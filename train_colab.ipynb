{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›ï¸ LexGuard - Fine-tuning Llama-3-8B for Legal Metadata Extraction\n",
    "\n",
    "This notebook fine-tunes **Llama-3-8B** on 500 labeled legal contracts from the CUAD dataset.\n",
    "\n",
    "### Steps:\n",
    "1. Install Unsloth\n",
    "2. Upload `finetune_data.jsonl`\n",
    "3. Load model & train\n",
    "4. Evaluate on test set\n",
    "5. Save model\n",
    "\n",
    "> âš ï¸ **Before running:** Go to `Runtime â†’ Change runtime type â†’ T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload your `finetune_data.jsonl` file\n",
    "\n",
    "Run this cell, then click **Choose Files** and select `finetune_data.jsonl` from your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "print('âœ… File uploaded:', list(uploaded.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Split Dataset (70% Train / 30% Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the JSONL file\n",
    "full_dataset = load_dataset('json', data_files='finetune_data.jsonl', split='train')\n",
    "print(f'Total examples: {len(full_dataset)}')\n",
    "\n",
    "# Split into 70% train / 30% test\n",
    "split = full_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train_dataset = split['train']\n",
    "test_dataset  = split['test']\n",
    "\n",
    "print(f'Train set: {len(train_dataset)} examples')\n",
    "print(f'Test set : {len(test_dataset)} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Model with Unsloth (4-bit quantized for T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096  # Supports up to 4096 tokens\n",
    "dtype = None           # Auto-detect (float16 for T4)\n",
    "load_in_4bit = True    # Required for T4 GPU (16GB VRAM)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = 'unsloth/Meta-Llama-3-8B-Instruct-bnb-4bit',\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "print('âœ… Model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Apply LoRA (makes training fast & memory-efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,                    # LoRA rank (16 is a good balance)\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    "                      'gate_proj', 'up_proj', 'down_proj'],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = 'none',\n",
    "    use_gradient_checkpointing = 'unsloth',\n",
    "    random_state = 42,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "print('âœ… LoRA applied!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Format Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = 'llama-3',\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples['messages']\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "             for convo in convos]\n",
    "    return {'text': texts}\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "test_dataset  = test_dataset.map(formatting_prompts_func, batched=True)\n",
    "print('âœ… Dataset formatted!')\n",
    "print('Sample:')\n",
    "print(train_dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train! ðŸš€\n",
    "\n",
    "This will take **2-4 hours on a free T4 GPU**. You can leave it running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = 'text',\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2,       # Train for 2 full passes over the data\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = 'adamw_8bit',\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = 'linear',\n",
    "        seed = 42,\n",
    "        output_dir = 'lexguard_outputs',\n",
    "        report_to = 'none',\n",
    "    ),\n",
    ")\n",
    "\n",
    "print('ðŸš€ Starting training...')\n",
    "trainer_stats = trainer.train()\n",
    "print('âœ… Training complete!')\n",
    "print(f'   Training loss: {trainer_stats.training_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate with LLM Judge (Groq API)\n",
    "\n",
    "Thay vÃ¬ so sÃ¡nh exact string (quÃ¡ kháº¯t khe), mÃ¬nh dÃ¹ng **LLM judge** Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ ngá»¯ nghÄ©a.\n",
    "VÃ­ dá»¥: `\"5/8/14\"` vÃ  `\"May 8, 2014\"` sáº½ Ä‘Æ°á»£c judge coi lÃ  **match**.\n",
    "\n",
    "> Báº¡n cáº§n **Groq API key** miá»…n phÃ­ táº¡i [console.groq.com](https://console.groq.com)\n",
    "> Äiá»n API key vÃ o biáº¿n `GROQ_API_KEY` á»Ÿ cell bÃªn dÆ°á»›i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq -q\n",
    "print(\"Groq installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\nimport torch\nfrom groq import Groq\nfrom unsloth import FastLanguageModel\n\n# =====================================================\n# TODO: Nháº­p Groq API key cá»§a báº¡n vÃ o Ä‘Ã¢y\nGROQ_API_KEY = \"gsk_YOUR_KEY_HERE\"\n# =====================================================\n\ngroq_client = Groq(api_key=GROQ_API_KEY)\nJUDGE_MODEL  = \"openai/gpt-oss-120b\"\nJUDGE_SYSTEM = (\n    \"You are an expert evaluator for legal contract metadata extraction. \"\n    \"Given extracted values and ground truth values, decide if they semantically match. \"\n    \"Minor formatting differences (date formats, capitalization, brackets) should be IGNORED. \"\n    'Output ONLY valid JSON where each key is the field name: '\n    '{\"Field Name\": {\"match\": true/false, \"reasoning\": \"one line\"}}'\n)\n\ndef judge_all_fields(extracted, ground_truth):\n    lines = [\"Evaluate each field extraction below:\\n\"]\n    for field, gt_val in ground_truth.items():\n        pred_val = extracted.get(field, \"None mentioned\")\n        lines.append(f'Field: \"{field}\"\\n  Extracted   : {pred_val}\\n  Ground Truth: {gt_val}\\n')\n    user_msg = \"\\n\".join(lines)\n    for attempt in range(3):\n        try:\n            resp = groq_client.chat.completions.create(\n                model=JUDGE_MODEL,\n                messages=[\n                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n                    {\"role\": \"user\",   \"content\": user_msg}\n                ],\n                response_format={\"type\": \"json_object\"},\n                temperature=0,\n            )\n            return json.loads(resp.choices[0].message.content)\n        except Exception as e:\n            print(f\"  Judge error (attempt {attempt+1}): {e}. Waiting 3s...\")\n            time.sleep(3)\n    return {}\n\n# â”€â”€ Main Evaluation Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nFastLanguageModel.for_inference(model)\n\nnum_samples    = min(20, len(test_dataset))\ntotal_matches  = 0\ntotal_mismatches = 0\n\nprint(f\"Evaluating {num_samples} test examples with LLM Judge...\\n\")\n\nfor i in range(num_samples):\n    example  = test_dataset[i]\n    messages = example[\"messages\"]\n    try:\n        ground_truth = json.loads(messages[-1][\"content\"])\n    except Exception:\n        continue\n\n    doc_name = ground_truth.get(\"Document Name\", f\"Example {i+1}\")\n    print(f\"[{i+1}/{num_samples}] {str(doc_name)[:60]}\")\n\n    # Generate model output\n    prompt_messages = messages[:-1]\n    inputs = tokenizer.apply_chat_template(\n        prompt_messages, tokenize=True,\n        add_generation_prompt=True, return_tensors=\"pt\"\n    ).to(\"cuda\")\n    with torch.no_grad():\n        out = model.generate(\n            input_ids=inputs, max_new_tokens=2048,\n            temperature=0.1, do_sample=False\n        )\n    generated = tokenizer.decode(out[0][inputs.shape[-1]:], skip_special_tokens=True)\n\n    # Parse model JSON\n    try:\n        s = generated.find(\"{\")\n        e = generated.rfind(\"}\") + 1\n        predicted = json.loads(generated[s:e])\n    except Exception:\n        print(\"  Could not parse model JSON, skipping...\")\n        total_mismatches += len(ground_truth)\n        continue\n\n    # Ask LLM Judge\n    judge_results = judge_all_fields(predicted, ground_truth)\n    ex_matches    = sum(1 for r in judge_results.values()\n                        if isinstance(r, dict) and r.get(\"match\", False))\n    ex_mismatches = len(ground_truth) - ex_matches\n    total_matches    += ex_matches\n    total_mismatches += ex_mismatches\n    print(f\"  Matches: {ex_matches}  |  Mismatches: {ex_mismatches}\")\n\ntotal_fields = total_matches + total_mismatches\naccuracy = (total_matches / total_fields * 100) if total_fields > 0 else 0\nprint(f\"\\nFinal Accuracy (LLM Judge): {total_matches}/{total_fields} = {accuracy:.1f}%\")\nprint(\"(Semantic accuracy -- much more meaningful than exact string matching!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save the fine-tuned model\n",
    "\n",
    "Choose one of the options below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Save as GGUF (for running locally on Mac with Ollama / LM Studio)\n",
    "# This creates a file you can download and use on your Mac without internet!\n",
    "model.save_pretrained_gguf('lexguard_model', tokenizer, quantization_method='q4_k_m')\n",
    "print('âœ… GGUF model saved to lexguard_model/')\n",
    "print('   Download this folder and use with Ollama or LM Studio on your Mac.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Push to Hugging Face Hub (to use as an API like Groq)\n",
    "# You need a free HuggingFace account and token from https://huggingface.co/settings/tokens\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token='YOUR_HF_TOKEN_HERE')\n",
    "\n",
    "# model.push_to_hub('your-username/lexguard-llama3-8b', tokenizer=tokenizer)\n",
    "# print('âœ… Model pushed to HuggingFace Hub!')\n",
    "\n",
    "print('Uncomment the code above and add your HuggingFace token to use this option.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Done!\n",
    "\n",
    "Your **LexGuard** model is now fine-tuned for legal metadata extraction!\n",
    "\n",
    "**Next steps:**\n",
    "- Download the GGUF file â†’ use with **Ollama** on your Mac (free, offline!)\n",
    "- Or push to HuggingFace Hub â†’ replace Groq API in `pipeline.py` with your new model"
   ]
  }
 ]
}